# GitLab Configuration
GITLAB_URL=https://gitlab.example.com
GITLAB_PAT=glpat-xxxxxxxxxxxxxxxxxxxx

# LLM Configuration (for chat/inference)
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o
# OpenAI Embeddings (when EMBEDDING_PROVIDER=openai)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# Optional: Use OpenAI-compatible API (e.g., local LLM server)
# OPENAI_BASE_URL=http://localhost:11434/v1

# Embedding Configuration
# Provider: "openai" or "local"
EMBEDDING_PROVIDER=openai

# Local Embeddings (when EMBEDDING_PROVIDER=local)
# Uses sentence-transformers via t2v-transformers container
# To enable: docker compose --profile local-embeddings up -d
LOCAL_EMBEDDING_URL=http://t2v-transformers:8080
LOCAL_EMBEDDING_DIMENSION=384

# Enable CUDA for local embeddings (if GPU available)
ENABLE_CUDA=0

# Database Configuration
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=gitlab_chat
POSTGRES_USER=gitlab_chat
POSTGRES_PASSWORD=change_me_in_production

# Qdrant Configuration
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# Redis Configuration
REDIS_URL=redis://redis:6379/0

# Application Settings
CHUNK_SIZE=512
CHUNK_OVERLAP=50
TOP_K_RESULTS=10

# Celery Worker Settings
CELERY_CONCURRENCY=4

# Frontend (for Next.js)
NEXT_PUBLIC_API_URL=http://localhost:8000
